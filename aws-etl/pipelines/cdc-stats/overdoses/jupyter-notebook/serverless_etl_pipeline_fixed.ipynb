{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e18eb8",
   "metadata": {},
   "source": [
    "# Serverless Cloud ETL Pipelines in Modern GIS Development\n",
    "\n",
    "In modern Geographic Information Systems (GIS) development, real-time data acquisition and dynamic visualization are key components. Cloud-based ETL (Extract, Transform, Load) pipelines enable the integration of external data (e.g., public health or census data) into geospatial applications by automating data ingestion, processing, and delivery. Serverless architectures, such as AWS Lambda, simplify infrastructure management, allowing scalable and cost-efficient data workflows. This notebook demonstrates a serverless ETL pipeline that fetches CDC data and ESRI Feature Services, performs geospatial and statistical transformations, and stores the results in Amazon S3.\n",
    "\n",
    "Currently, they store this in json format. This can be flattened or converted to Power BI friendly formats (csv, parquet) inside the S3 data lake. This allows ingestion of data into BI platforms, while also staging it for ESRI feature service REST API updates. \n",
    "\n",
    "In this notebook, we will walk through each component of the provided Lambda function code and describe its role in the ETL pipeline. This includes environment variable resolution, ESRI authentication, data fetching, transformation, standardization, and cloud storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d84734",
   "metadata": {},
   "source": [
    "## Section 1: Importing Required Libraries\n",
    "These libraries support HTTP communication, AWS resource interaction, structured logging, and in-memory file I/O. Pandas is used for efficient tabular data processing and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import traceback as tb\n",
    "import io\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00689a49",
   "metadata": {},
   "source": [
    "## Section 2: Constants and AWS Client Setup\n",
    "Defines the ArcGIS token URL and sets up AWS SDK clients for Systems Manager (SSM) and Lambda. The logger captures key events during execution for debugging and observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e360d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_URL = \"https://www.arcgis.com/sharing/rest/oauth2/token/\"\n",
    "ssm_client = boto3.client(\"ssm\")\n",
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def log(message):\n",
    "    logger.info(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da175c9e",
   "metadata": {},
   "source": [
    "## Section 3: Environment Variable Accessor Class\n",
    "Responsible for retrieving environment variable names from the Lambda environment and resolving them to actual values using AWS SSM Parameter Store. These parameters contain credentials and URLs required for API communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a37ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnivronmentAccessor():\n",
    "    def __init__(self):\n",
    "        self.environments_dictionary = self.__build_envparam_dictionary()\n",
    "\n",
    "    def __build_envparam_dictionary(self):\n",
    "        user_cred_esri = os.environ.get(\"ARCGIS_CREDENTIALS_ARN\")\n",
    "        user_secret_esri = os.environ.get(\"ARCGIS_SECRET_ARN\")\n",
    "        scdph_usaboundary_esri = os.environ.get(\"ARCGIS_USA_POP\")\n",
    "        cdc_covidalloction = os.environ.get(\"CDC_COVID_ALLOCATION\")\n",
    "        cdc_overdoses = os.environ.get(\"ARCGIS_CDC_OVERDOSE\")\n",
    "        scdph_countyboundary_esri = os.environ.get(\"ARCGIS_COUNTIES_POP\")\n",
    "        container_params = os.environ.get(\"ARCGIS_CONTAINER\")\n",
    "        cdc_apis = os.environ.get(\"CDC_API_DICT\")\n",
    "\n",
    "        return {\n",
    "            \"user_cred_esri\": ssm_client.get_parameter(Name=user_cred_esri, WithDecryption=True)[\"Parameter\"][\"Value\"],\n",
    "            \"user_secret_esri\": ssm_client.get_parameter(Name=user_secret_esri, WithDecryption=True)[\"Parameter\"][\"Value\"],\n",
    "            \"scdph_usaboundary_esri\": ssm_client.get_parameter(Name=scdph_usaboundary_esri, WithDecryption=True)[\"Parameter\"][\"Value\"],\n",
    "            \"scdph_counties_esri\": ssm_client.get_parameter(Name=scdph_countyboundary_esri, WithDecryption=True)[\"Parameter\"][\"Value\"],\n",
    "            \"cdc_covidalloction\": ssm_client.get_parameter(Name=cdc_covidalloction, WithDecryption=True)[\"Parameter\"][\"Value\"],\n",
    "            \"cdc_overdoses\": ssm_client.get_parameter(Name=cdc_overdoses, WithDecryption=True)[\"Parameter\"][\"Value\"],\n",
    "            \"arcgis_container_params\": ssm_client.get_parameter(Name=container_params, WithDecryption=True)[\"Parameter\"][\"Value\"],\n",
    "            \"cdc_apis\": ssm_client.get_parameter(Name=cdc_apis, WithDecryption=True)[\"Parameter\"][\"Value\"]\n",
    "        }\n",
    "\n",
    "environment_accessor = EnivronmentAccessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514407a1",
   "metadata": {},
   "source": [
    "## Section 4: ESRI Authentication\n",
    "ArcGIS uses OAuth2 for API authentication. These functions obtain and return a short-lived access token used for secure data requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73577de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arcgis_credentials():\n",
    "    client_id = environment_accessor.environments_dictionary.get(\"user_cred_esri\")\n",
    "    client_secret = environment_accessor.environments_dictionary.get(\"user_secret_esri\")\n",
    "    if not client_id:\n",
    "        raise ValueError(\"Environment variable 'ARCGIS_CREDENTIALS_ARN' is not set.\")\n",
    "    if not client_secret:\n",
    "        raise ValueError(\"Environment variable 'ARCGIS_SECRET_ARN' is not set.\")\n",
    "    return client_id, client_secret\n",
    "\n",
    "def get_esri_token():\n",
    "    client_id, client_secret = get_arcgis_credentials()\n",
    "    params = {\n",
    "        \"client_id\": client_id,\n",
    "        \"client_secret\": client_secret,\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"f\": \"json\"\n",
    "    }\n",
    "    response = requests.post(TOKEN_URL, data=params)\n",
    "    response.raise_for_status()\n",
    "    token_data = response.json()\n",
    "    token = token_data.get(\"access_token\")\n",
    "    if not token:\n",
    "        raise Exception(f\"Token not found in response: {token_data}\")\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c289649",
   "metadata": {},
   "source": [
    "## Section 5: Fetch GIS Data from ArcGIS Cloud Based Enterprise Environment\n",
    "\n",
    "Uses environment parameters from step 3 to access feature service url and build query parameters for reading GIS data into a pandas dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca4508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_esridata_with_token(access_token: str, environment_url:str, build_geom:bool=False):\n",
    "    if not environment_url and access_token:\n",
    "        raise ValueError(\"Environment variable 'ARCGIS_USA_POP' is not set or empty.\")\n",
    "    params = {\n",
    "        \"where\": \"1=1\",\n",
    "        \"token\": access_token,\n",
    "        \"outFields\": '*',\n",
    "        \"f\": \"json\"\n",
    "    }\n",
    "    response = requests.get(environment_url, params=params)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    try:\n",
    "        data = response.json()\n",
    "        attributes_df = pd.json_normalize([feature[\"attributes\"] for feature in data[\"features\"]])\n",
    "        attributes_df = attributes_df.drop(columns=[col for col in attributes_df.columns if 'OBJECTID' in col], errors='ignore')\n",
    "\n",
    "        if build_geom:\n",
    "            geometry_df = pd.json_normalize([feature.get(\"geometry\", {}) for feature in data[\"features\"]])\n",
    "\n",
    "            esri_feature_df = pd.concat([attributes_df, geometry_df], axis=1)\n",
    "        else:\n",
    "            esri_feature_df = attributes_df\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"raw_response\": response.text}\n",
    "\n",
    "    return esri_feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f039d04",
   "metadata": {},
   "source": [
    "## Step 6 Normalize and Standardize for Spatial Statistics\n",
    "\n",
    "This takes two dataframes and merges them using specified key columns (merge_key_df1 and merge_key_df2) to align population data (df2) with a primary dataset (df1), such as geographic regions. It uses the population_column to access population values and the cdc_sta column to access raw counts or statistics (e.g., number of cases). The function then calculates a standardized rate per a fixed unit of population, defined by the normalizer (e.g., per 1,000 or 100,000 people), and stores the result in a new column named statistic_final_column. This process allows for population-based comparison across regions while preserving all original columns and eliminating duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c0fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_cdc_overdose_data_to_gdb(cdc_api_url:str=None):\n",
    "    if not cdc_api_url:\n",
    "        raise ValueError(\"Environment varibles for cdc apis is not set or empty.\")\n",
    "\n",
    "    cdc_overdose_df = build_cdc_dataframe(cdc_api_url)\n",
    "    print(\"Columns:\", cdc_overdose_df.columns.tolist())\n",
    "    print(\"Data types:\\n\", cdc_overdose_df.dtypes)\n",
    "\n",
    "    cdc_overdose_df[\"provisional_drug_overdose\"] = cdc_overdose_df[\"provisional_drug_overdose\"].astype(float)\n",
    "\n",
    "    cdc_overdose_df[\"fips_join\"] = cdc_overdose_df[\"fips\"].apply(lambda x: '0' + str(x) if len(str(x)) < 5 else str(x))\n",
    "\n",
    "    aggregated_overdose = cdc_overdose_df.groupby(\n",
    "        [\"year\", \"countyname\", \"state_name\", \"fips_join\"]\n",
    "    )[\"provisional_drug_overdose\"].sum().reset_index()\n",
    "\n",
    "    return aggregated_overdose\n",
    "\n",
    "def standardize_statistics_populationbased(merge_key_df1, merge_key_df2, df1, df2, normalizer, population_column, cdc_sta, statistic_final_column):\n",
    "    \"\"\"\n",
    "    Merge two dataframes based on a common key and perform statistical calculations,\n",
    "    while retaining all columns from both dataframes.\n",
    "\n",
    "    Parameters:\n",
    "    - merge_key_df1 (str): The column name to use as the key for merging in df1.\n",
    "    - merge_key_df2 (str): The column name to use as the key for merging in df2.\n",
    "    - df1 (pd.DataFrame): The first dataframe to merge.\n",
    "    - df2 (pd.DataFrame): The second dataframe to merge.\n",
    "    - normalizer (int): The value to divide the cdc_sta column by. Standard per x, example 1000.\n",
    "    - population_column (str): The column name containing the population values.\n",
    "    - cdc_sta (str): The column name containing the statistic values.\n",
    "    - statistic_final_column (str): The name of the new column to store the calculated statistic.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The merged dataframe with all original columns and the calculated statistic.\n",
    "    \"\"\"\n",
    "    df1 = df1.drop_duplicates(subset=[merge_key_df1])\n",
    "    df1[merge_key_df1] = df1[merge_key_df1].astype(str)\n",
    "    df2[merge_key_df2] = df2[merge_key_df2].astype(str)\n",
    "    \n",
    "    # Perform the merge, keeping all columns\n",
    "    merged_df = pd.merge(df1, df2, left_on=merge_key_df1, right_on=merge_key_df2, how='left', suffixes=('_df1', '_df2'))\n",
    "    # Drop any column that contains 'OBJECTID' in its name\n",
    "    merged_df.drop(columns=[col for col in merged_df.columns if 'OBJECTID' in col], inplace=True)\n",
    "\n",
    "    \n",
    "    # Compute the standardized statistic\n",
    "    merged_df[statistic_final_column] = (merged_df[cdc_sta] / merged_df[population_column]) * normalizer\n",
    "\n",
    "    final_df = drop_case_insensitive_duplicates(merged_df)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1bf9ea",
   "metadata": {},
   "source": [
    "## Step 7 Upload Dataframe Produced with Spatial Statistics to AWS S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d6d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(data, file_name):\n",
    "    try:\n",
    "        # Ensure input is a DataFrame\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            raise ValueError(\"Data must be a pandas DataFrame\")\n",
    "\n",
    "        # Create a byte stream buffer\n",
    "        buffer = io.BytesIO()\n",
    "\n",
    "        # Convert DataFrame to JSON and write it to buffer\n",
    "        json_str = data.to_json(orient=\"records\", indent=4)  # Convert DF to JSON string\n",
    "        buffer.write(json_str.encode())  # Encode as bytes\n",
    "        \n",
    "        # Reset buffer position to start\n",
    "        buffer.seek(0)\n",
    "\n",
    "        # Debugging: Print buffer content\n",
    "        print(\"File content in buffer:\", buffer.getvalue().decode())\n",
    "\n",
    "        # Initialize S3 client\n",
    "        s3 = boto3.client(\"s3\", region_name=\"us-east-2\")\n",
    "        bucket_name = \"scdphn-demo-bucket\"\n",
    "        object_key = f\"{file_name}\"\n",
    "\n",
    "        try:\n",
    "            s3.head_object(Bucket=bucket_name, Key=object_key)\n",
    "            file_exists = True\n",
    "        except s3.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == \"404\":\n",
    "                file_exists = False\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        if file_exists:\n",
    "            copy_source = {'Bucket': bucket_name, 'Key': object_key}\n",
    "            \n",
    "            s3.copy_object(\n",
    "                CopySource=copy_source,\n",
    "                Bucket=bucket_name,\n",
    "                Key=object_key,\n",
    "                MetadataDirective=\"REPLACE\" \n",
    "            )\n",
    "            print(f\"Updated {object_key} in S3 (copied to itself).\")\n",
    "        else:\n",
    "            s3.put_object(Bucket=bucket_name, Key=object_key, Body=buffer.getvalue())\n",
    "            print(f\"Uploaded {object_key} to S3.\")\n",
    "\n",
    "        return {\"status\": \"Success\", \"message\": f\"Uploaded or updated {object_key} in S3\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418431e6",
   "metadata": {},
   "source": [
    "## Run Process\n",
    "lambda_handler is the generic function that lambda uses to run process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71921fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_handler(event, context):\n",
    "    try:\n",
    "        # Check if the event contains the \"ssm_update\" key\n",
    "        if \"ssm_update\" not in event:\n",
    "            raise KeyError(\"Missing 'ssm_update' key in event.\")\n",
    "\n",
    "        param_name = \"cdc-container-param\"\n",
    "        resp = store_json_in_ssm(param_name, event)\n",
    "\n",
    "        ssm_updates = event[\"ssm_update\"]\n",
    "\n",
    "        if not isinstance(ssm_updates, list):\n",
    "            raise ValueError(\"'ssm_update' must be a list of directory-file objects.\")\n",
    "\n",
    "        directory_files_mappings = []\n",
    "        access_token = get_esri_token()\n",
    "\n",
    "        cdc_apis_str = environment_accessor.environments_dictionary[\"cdc_apis\"]\n",
    "        try:\n",
    "            total_cdcapis = json.loads(cdc_apis_str)\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\n",
    "                f\"CDC_APIS_DICT from SSM is not valid JSON: {cdc_apis_str}\"\n",
    "            )\n",
    "        stats=[]\n",
    "        for update in ssm_updates:\n",
    "            if not isinstance(update, dict):\n",
    "                raise ValueError(\"Each item in 'ssm_update' must be a dictionary.\")\n",
    "\n",
    "            # Handle dynamic keys (directory names) in the update object\n",
    "            for directory, file in update.items():\n",
    "                if not isinstance(file, str):\n",
    "                    raise ValueError(f\"Files for directory '{directory}' must be a list.\")\n",
    "                if directory in total_cdcapis:\n",
    "\n",
    "                    cdc_api_url_lookup = total_cdcapis[directory]\n",
    "\n",
    "                    if \"esri_boundary_apiurl\" not in event:\n",
    "                        raise KeyError(\"Missing 'esri_boundary_apiurl' key in event.\")\n",
    "                    env_dictkey = event[\"esri_boundary_apiurl\"]\n",
    "                    if not env_dictkey:\n",
    "                        raise ValueError(\"No env_dictkey\")\n",
    "                    env_url = environment_accessor.environments_dictionary[env_dictkey]\n",
    "                    if not env_url:\n",
    "                        raise ValueError(f\"Environment variable '{env_dictkey}' is not set or empty.\")\n",
    "\n",
    "                    esri_feature_data = fetch_esridata_with_token(\n",
    "                        access_token=access_token,\n",
    "                        environment_url=env_url\n",
    "                    )\n",
    "\n",
    "                    if directory == 'CountiesOverdosePopulationData':\n",
    "                        cdc_overdoses_df = normalize_cdc_overdose_data_to_gdb(\n",
    "                            cdc_api_url_lookup\n",
    "                        )\n",
    "                        standardized_stats_df = standardize_statistics_populationbased(\n",
    "                            merge_key_df1=\"FIPS\",\n",
    "                            merge_key_df2=\"fips_join\",\n",
    "                            df1=esri_feature_data,\n",
    "                            df2=cdc_overdoses_df,\n",
    "                            normalizer=100000,\n",
    "                            population_column=\"POPULATION\",\n",
    "                            cdc_sta=\"provisional_drug_overdose\",\n",
    "                            statistic_final_column=\"overdose_rate_per_100k\"\n",
    "                        )\n",
    "                        stats.append({\n",
    "                                \"directory\": directory,\n",
    "                                \"file\": file,\n",
    "                                \"rowCount\": len(standardized_stats_df)\n",
    "                            })\n",
    "                        upload_to_s3(standardized_stats_df, f\"{directory}/{file}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Directory '{directory}' not found in CDC_APIS_DICT.\")\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"statusCode\": 200,\n",
    "            \"body\": stats\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"statusCode\": 500,\n",
    "            \"body\": json.dumps({\"error\": str(e), \"traceback\": tb.format_exc()})\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lambda_handler(None, None)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
